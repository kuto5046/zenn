---
title: "強化学習はLLMの新しい能力を『発見』しているのか？"
emoji: "💡"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["RL", "LLM"]
published: true
---
この記事は[LLM・LLM活用 Advent Calendar 2025](https://qiita.com/advent-calendar/2025/large-language-model)(シリーズ3)の19日目の記事です

## LLM研究におけるRLの躍進

2025年、LLM研究において強化学習(Reinforcement Learning, RL)の役割が大きくなりつつあります。中でも話題となったのが、2025年1月に発表されたDeepSeek-R1です。

https://arxiv.org/abs/2501.12948

DeepSeek-R1は、価値評価モデルを不要とするGRPOというRLアルゴリズムや、答えが一意に定まる問題に対して検証可能な報酬を利用することで報酬モデルを取り除いて学習コストを下げるといった具体策を提示しました。これらの手法により、RLがLLMの推論・汎化能力を飛躍的に向上させることを実証しています。
さらに、その過程ではahaモーメントと呼ばれる興味深い現象も観測されています。これはLLM自身が推論過程で気づきを得る事象であり、RLによって新しい推論能力を獲得しているように見えます。
しかし、RLが本当に新しい推論能力を獲得しているのかについては議論があり、トピックとして面白いなと思ったので紹介していきます。

## RLの役割は『先鋭化』か『発見』か

RLがLLMに与える影響は『先鋭化(Sharpening)』と『発見(Discovery)』のどちらなのかという議論が存在します。
- 先鋭化: ベースモデルが既に持っている能力を強調する役割
- 発見: ベースモデルにはない新しい能力を身につける役割

なお、本記事では指示チューニングや選好チューニングを施していない事前学習済みモデルをベースモデルと呼ぶことにします。
先鋭化を主張する立場の研究では、RLは一見新しい能力を獲得しているように見えるものの、実際には**ベースモデルが潜在的に持っている能力を表出させているだけではないか**と考えられています。
この記事では、それぞれの主張を支持する論文を以下のサーベイ論文から抜粋して紹介します。
https://arxiv.org/abs/2509.08827

## 先鋭化を主張する研究

### RLは推論時のサンプリングを効率化しているに過ぎない

https://arxiv.org/abs/2504.13837
こちらはDeepSeek-R1に端を発する強化学習と検証可能な報酬（RL with Verifiable Rewards, RLVR）がLLMの推論能力を向上させるという一般的な認識に疑問を投げかけた研究です。RLVRとは、数学の問題に正解した場合に与える正解報酬のようなルールベースで与えることができる報酬を指します。

この研究では**RLVRは新しい推論能力を生み出しているのではなく、もともと出せる答えの中から正解を効率的に選び出しているだけではないか**、という仮説を検証しています。
もう少し噛み砕くと、下記イメージ図のProblem Aのようにベースモデルが8回推論をして2回正解を引き当てる場合に2回の推論で2回とも正解を引き当てるようになるのがRLの役割で、これは新しい推論能力を獲得しているわけではないという仮説です。
![alt text](/images/41c8a08421e3a1/image1.png)

この仮説を検証するため、ベースモデルに対してRLを実施後に、数学、コーディング、視覚推論といった複数のベンチマークでpass@Kという指標を用いた実験を実施しました。
::: message
pass@Kとは、K回推論を行い1回でも正解すれば正解とする指標です。
例えば、pass@1は1回の推論で正解できるかどうか(つまり一般的な正答率)を示し、pass@10は10回の推論で1回でも正解できるかどうかを示すものです。
:::
![alt text](/images/41c8a08421e3a1/image2.png)
上のグラフは複数の数学ベンチマークと複数のモデル種別ごとに、ベースモデルとRLモデルのpass@Kを比較した結果です。これを見ると、pass@1ではRL(赤)が優れているものの、推論回数Kが大きくなると徐々にベースモデル(緑)も性能が高くなり、最終的に性能が逆転する傾向が見られました。
これは、**現在のRLVR手法はLLMの真に新しい推論能力を生み出してはおらず、ベースモデルのサンプリング効率を改善しているに留まっている**ことを示唆しています。

### サンプリングの工夫でRLと同等の性能を実現

https://arxiv.org/abs/2510.14901

こちらの研究でも上記と同様、RLがモデルに「新しい能力」を与えているのではなく、単にベースモデルが持つ知識分布を「先鋭化（Sharpening）」させているだけではないかという仮説に基づいています。この研究ではLLM出力の確率分布の先鋭化とサンプリングの工夫によってファインチューニングなしにpass@1指標でRLと同等以上の精度を実現可能であることを示しました。これは**RLによる推論性能の改善が先鋭化とサンプリングの工夫によって実現可能であり、RLは先鋭化しているに過ぎない**ことを示唆しています。
![](/images/41c8a08421e3a1/image3.png)


## 発見を主張する研究

ここまで紹介した研究を聞くと、「あれ、RLってベースモデルを先鋭化してるだけかも？」という気持ちになってきますが、それに反論する研究も存在しています。

### RLで長期学習することで新しい推論パターンを発見

https://arxiv.org/abs/2505.24864

先鋭化を主張する研究では、推論回数Kを増やせばベースモデルがRLの性能を上回る事象が確認されていましたが、ProRLでは適切なRL手法と十分な学習ステップ数があれば、ベースモデルが全く解けないタスクでもRLによって解法を発見できることを実証しました。  

具体的には、学習の安定性のために[DAPO](https://arxiv.org/abs/2503.14476)という論文で紹介されている学習テクニックを採用しています。また長期間の学習を行う際に、参照モデルを初期のまま固定しておくとKLダイバージェンスの乖離が徐々に大きくなり学習が停滞する課題があるため参照モデルを定期的に更新するといった学習上の工夫を行っています。
![alt text](/images/41c8a08421e3a1/image4.png)
上のグラフで示す複数のベンチマークにおいてpass@Kを、ベースモデル、RLの学習途中、RLの学習完了後の3つで比較したところ、タスクによって異なる傾向があることがわかりました。

#### Diminish:ベースモデルがすでに得意としているタスク
- RLによりpass@1のスコアは向上するが、Kが大きいとベースモデルと性能が逆転している
- 先鋭化を主張している研究結果とも一致する

#### Sustained: 複雑なタスク
- 学習が進むにつれて性能が上がり続けている(オレンジ < 緑)
- Kが大きくなってもRLモデルの方がベースモデルよりも性能が高い

このようにタスクによっては、先鋭化を主張する研究に反論する結果が得られていることがわかります。Sustainedの結果を批判的に見るとK=256で終了しているからそのように見えるだけでKをさらに大きくすれば、ベースモデルが逆転するのでは？という疑念もありますが、本研究で注目すべきは下に示すタスクのような事例です。
![](/images/41c8a08421e3a1/image7.png)
ベースモデルの分布外である一部のタスクでは推論回数Kを増やしてもベースモデルの正答率が常に0である一方でRLモデルは学習によって正答率が上昇していることがわかります。

このように本研究では、難易度が比較的低めのタスクにおいて先鋭化の傾向があることを認めつつも、**難易度が高いタスクにおいては学習がうまく進むように工夫することによってベースモデルでは解けない新たな解法を発見している**ことを示唆しています。
ここでは特に重要と思われる研究1つに絞って紹介しましたが、そのほかにも発見を主張する研究は存在しています。

## 現時点での暫定的な結論
このトピックについては、まだ明確な決着がついているわけではありませんが、ここで紹介した研究の周囲も含めて見渡すと以下のような整理に現状ではなってると考えています。
- RLの役割は先鋭化 or 発見の2者択一ではなく両方の役割を兼ね備えている
- ベースモデルでも推論回数を増やせば解くことが可能なタスクにおいては先鋭化の役割が大きい傾向にある
- ベースモデルでも解くことが難しいような一定の条件を満たすタスクにおいては、学習条件を適切に整えることで発見も発生しうる
- いずれの場合でもRLによる性能向上はベースモデルの性能にはある程度依存している

## 個人的な解釈と学び
ここからはこのトピックに関して私が考えたことを述べてみます。あくまで現時点での個人的な解釈であり、誤っている点もあるかもしれないためご了承ください。

### なぜ先鋭化の傾向が出やすいのか

周辺の研究を見ていると、発見が発生してそうな事例もあるものの、RLは先鋭化の傾向が出やすい印象を持っています。発見ではなく先鋭化の傾向が出やすいのは、以下の2つの理由があると考えてます。
- 難易度が低いタスクの場合はベースモデルでも解けるためそもそも発見は起こらない。その結果、先鋭化としての役割が観測されている
- 逆に難易度が高いタスクは、アウトカム報酬のみでは報酬が獲得できずRLの学習が進みにくい。そのためベースモデルやタスクの選定、学習条件の工夫をしないと発見が起こりにくい

### RLはどういう目的で使うべきか
RLを単なる性能向上目的として扱うのではなく、ここで議論されたような先鋭化と発見という2つの役割に分担して考えてみます。

#### 先鋭化を目的とする場合
ベースモデルでも推論回数を増やせば解くことができるタスクに対して、**少ない推論回数で正解に辿り着きたい場合**にRLは有効な選択肢となると思います。推論回数(推論時間)を減らしたいといったケースは多く存在すると考えられるため、先鋭化としての役割も重要であると考えています。

#### 発見を目的とする場合
1. ベースモデルでは解けない難しいタスクへの適用

記事の中でも紹介したように難易度が高いタスクに対して学習条件を工夫することで発見的な性能向上が発生する可能性があります。
難易度が高いタスクでは報酬獲得が難しいのが最大の課題であるため、以下のような工夫が必要になると考えています。これらはRL研究でずっと取り組まれている普遍的な課題でもあります。
- 方策の探索を促進する(ex;エントロピー正則化)
- RLアルゴリズムの工夫で学習効率を高める
- プロセス報酬を導入してアウトカム報酬獲得のための補助を行う
- SFT(教師ありファインチューニング)をRLの前に実施して必要な知識やルールを事前に学習させる


2. マルチステップのエージェントタスクへの適用

LLMが取り組んでいた従来タスクはユーザプロンプトを入力として最適な文字列応答を生成するシングルステップのタスクが主でしたが、AIエージェントの登場に伴いマルチステップのタスクが増えてきています。ツール利用によって環境と相互作用して探索を行い、最適な行動を選択するエージェントタスクの学習は、本来のRLが得意とするタスクです。

実際、先鋭化を主張している論文内でも、「現行のRL手法では新能力創出に至っていない」という結論に加え、「将来的にRLによる新能力創出の可能性を引き出すには訓練のさらなるスケール化やマルチターンの環境インタラクションが鍵になる」と述べられています。
エージェント型強化学習(Agentic RL)については以前、周辺研究を調査した記事を書いたので興味があれば読んでみてください。
https://zenn.dev/kuto5046/articles/agentic_rl_2025

### RLとSFTどちらを採用すべきか
ここで取り上げた先鋭化や発見を達成する手法としてSFTを行う選択肢もあります。SFTとRLの役割の違いについてはSFTは『暗記』し、RLは『汎化』を促進するという考え方が下記の論文を中心に広まっています。
https://arxiv.org/abs/2501.17161
これはRLの方が優れているという主張ではなく両手法で学習の性質に違いがあるため、目的に応じて使い分けたり組み合わせて使おうということだと解釈してます。
新しい知識やルールを明示的にモデルに注入する目的ではSFTを使うのが良さそうで、目的関数の最大化を目指す問題設定ではRLが良さそうと個人的には考えています。

## おわりに
本記事では、LLMに対するRLの役割が「先鋭化」なのか「発見」なのかという議論を紹介しました。2025年はRLに再び注目が集まり、目覚ましい勢いで技術が進展した1年でした。2026年もLLMや強化学習の発展が楽しみです！
