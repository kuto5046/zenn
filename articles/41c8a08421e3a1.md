---
title: "強化学習はLLMの新しい能力を「発見」しているのか？"
emoji: "🌊"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["RL", "LLM"]
published: false
---


## LLM研究におけるRLの躍進

- 2025の強化学習(RL)の躍進は大きい。一番注目を集めたのは2025年1月に発表されたDeepSeek-R1
https://arxiv.org/abs/2501.12948

- DeepSeek-R1は、価値評価モデルを不要とするGRPOというRLアルゴリズムや、答えが一意に定まる問題に対して検証可能なルールベース報酬を用い、報酬モデルを取り除いて学習コストを下げるなどの具体策とともに、RLがLLMの推論・汎化能力を飛躍的に向上させることを示した。
- その過程でahaモーメントと呼ばれるLLM自身が気づきを得る内省の事象も観測されている。一見するとRLによって新しい推論能力を獲得してるように見える。
- こう言った流れを見るとLLMの新しい能力がRLによって獲得されているように見える。
- ただしここについてはいくつか議論があり面白い議題だったので紹介する。

## RLの役割は「先鋭化」か「発見」か

- RLの役割は「先鋭化(Sharpening)」か「発見(Discovery)」かという議論がある。
- 先鋭化: ベースモデルの能力を強調している
- 発見: ベースモデルにない能力を身につけている
- 事前学習済みモデルのことをベースモデルと呼ぶことにする。(指示チューニングや選好チューニングはしていないもの)
- 先鋭化の主張としてはRLは新しい能力を獲得しているように見えるがそれはベースモデルが潜在的に持っている能力を表出させているだけではないかというもの
- 明確に決着がついているわけではない。
- ここではそれぞれの主張をいくつかの論文を抜粋して紹介する
- 以下のサーベイ論文やDeepResearch結果で紹介されている論文を参照。客観性のあるまとめを目指している。
https://arxiv.org/abs/2509.08827
https://chatgpt.com/s/dr_69422745b394819189a75584356b2cb3
https://gemini.google.com/share/69887002d84c


## 先鋭化しているだけという主張
https://arxiv.org/abs/2504.13837
- この論文は、DeepSeek-R1に端を発する強化学習と検証可能な報酬（RLVR）がLLMの推論能力を向上させるという一般的な認識に疑問を投げかけたもの
- 補足すると検証可能な報酬とは数学の問題に正解した場合に与える正解報酬のようなルールベースで与えることができる報酬のこと
- RLVRは単に既存の推論パターンをより効率的にサンプリングしているだけなのではないか？という主張
![alt text](/images/41c8a08421e3a1/image1.png)

- これを検証するために数学、コード生成、視覚推論といった複数のベンチマークで実験を実施した。
- 下記は一例として複数の数学ベンチマークと複数のモデル種別ごとにベースモデルとRLモデルのpass@Kの比較を行ったもの。
- pass@KとはK回推論を行い1回でも正解すれば正解とする指標
- 検証の結果、pass@1ではRLが強いが、pass@K でKが大きくなるとベースモデルの方が性能が逆転する。
- つまり現在のRLVR手法はLLMの真に新しい推論能力を引き出すことに成功しておらず、ベースモデルのサンプリング効率を改善しているに留まっていることを示唆している。
- 今回取り上げた議題を問題提起した論文で重要な示唆を与えてくれる論文。

![alt text](/images/41c8a08421e3a1/image2.png)
- RLがモデルに「新しい能力」を与えているのではなく、単にベースモデルが持つ知識分布を「先鋭化（Sharpening）」させているだけではないかという仮説に基づく。
- 確率分布の先鋭化とサンプリングの工夫でRL相当の精度を実現することを試みた。
- 具体的にはLLMが出力する確率分布を先鋭化するようにべき乗した確率分布に変更し、MCMCを利用してサンプリングする手法を提案。
- 検証の結果、推論時のサンプリング方法を工夫することでRLを行わずとも複数のベンチマークにおいてpass@1指標においてRLと同等かそれ以上の精度を達成できることを示した。
https://arxiv.org/abs/2510.14901
![alt text](/images/41c8a08421e3a1/image3.png)

これらの主張は、現状のRLはあくまでサンプリング効率を高める先鋭化の役割が大きく、新しい能力の発見には至っていないというものである。

## 発見しているという主張
ここまではRLは先鋭化しているだけという主張の論文を紹介したが、それに反論する論文もある。

https://arxiv.org/abs/2505.24864
- 既存研究では、「RLはベースモデルに内在する正解を引き出しやすくするだけで、新しい推論能力を獲得するわけではないという議論がある。
- 本研究は、適切なRL手法と十分な学習時間があれば、ベースモデルが全く解けないタスクでもRLによって解法を発見できることを実証した。
- 具体的には本研究では学習効率を高めるためにGRPOを改良したDAPOの論文で紹介されているテクニックを採用している。(先鋭化を主張する論文はGRPO)
- また長期的な学習を行う際に、参照モデルを初期のまま固定しておくとKLダイバージェンスの乖離が徐々に大きくなり学習が停滞するため、参照モデルを定期的に更新することで学習の停滞を防ぎ、長期の学習を実現した。
- 複数のベンチマークにおいてpass@Kを、ベースモデル、RLの学習途中、RLの学習完了後で比較するとタスクによって異なる傾向があることがわかった。
- Diminishと記載されているベースモデルがすでに得意としているタスク(一部の数学タスク)においてはRLでpass@1が向上するもののKが大きいとベースモデルと性能が逆転している。これは先鋭化の主張とも一致する。
- 一方、Sustainedと記載されているコーディングのような複雑なタスクでは学習が進むにつれて性能が上がり続けておりKが大きくなってもベースモデルよりも性能が高いまま
- (この図だけ見るとSustainedでもKをもっと増やすと逆転しないか？というツッコミはありそう。)
- 特に右下のdiceタスクではベースモデルはKを増やしても正答率が0であるのに対してRLモデルは正答率が上昇している。これはベースモデルの探索空間にはない新たな推論パスを発見していることを示唆している。
- 十分に難しく、探索の余地があるタスクにおいては、長時間の強化学習がモデルの推論の境界を押し広げ、新たな解法を発見し続けることができることを示唆している

![alt text](/images/41c8a08421e3a1/image4.png)

https://arxiv.org/abs/2509.25123
- タスクに対する個別の知識は持っているが、組み合わせて推論する能力は持っていない状態を作り出した。
- ベースモデルに対して複数の文字列変換関数f(x)をFTして学習させる
- それぞれの関数を合成したものh(x)=f1(f2(x))を解くタスク
- 合成関数の組み合わせ数に応じて難易度が上がるようなタスク
![alt text](/images/41c8a08421e3a1/image6.png)

- 難易度が低いものに対しては先鋭化の傾向があるが、適切に難易度調整すれば新しい能力をRLによって獲得できる
- ここでの発見は、「新しい事実知識」の獲得ではなく、既存の知識断片を論理的に結合し、問題解決のための推論を発見させたと言える
![alt text](/images/41c8a08421e3a1/image5.png)

## 総括
- 2者択一ではなく両方兼ね備えている。つまり先鋭化も発見もしている
- 基本的には先鋭化の役割が大きい
- 学習条件を工夫すると、あるタスクにおいては発見も発生していそう

## 個人的解釈
ここからは個人的な解釈
- 発見ではなく先鋭化の傾向が出やすいのは以下の2つが理由と考えている
    - 難易度が低いタスクは、推論回数を増やせばベースモデルで解けるため、新しい推論は必要なく発見はそもそも起こり得ない
    - 難易度が高いタスクは、推論回数を増やしてもベースモデルでは解けない。これはRLの学習過程において報酬を獲得できないことを意味するためRLを適用しても学習が進まずやはり発見が起こらない


- そのため発見を促すには、難易度が高いタスクに対して、Pro RLにあるように学習条件を整えて学習効率を高めた状態で、学習ステップ数を十分増やせば確率的に報酬を獲得でき発見が促される
- 実際に先鋭化を主張している研究自体も、「現行のRL手法では新能力創出に至っていない」という結論に加え、**「将来的にこの可能性を引き出すには訓練のさらなるスケール化やマルチターンの環境インタラクションが鍵になる」**と述べている
- このことからAgentic RLのように探索が必要となる問題設定においてはRLの有効性は高いと考えている。ゲームAIでは全く能力がない状態からRLによって報酬信号だけで最適行動を学習するように探索が

- 先鋭化の役割が期待される場合
- ベースモデルでも推論回数を重ねれば解くことができるタスクに対してRLを適用する
- 少ない推論回数で正解に辿り着くことができる点(推論効率の改善)に価値がある

- 発見の役割が期待される場合
- ベースモデルでは解けないタスクに対してRLを適用する
    - 問題設定として、難易度が高く複雑性のあるタスク。
    - アルゴリズムの工夫で学習効率を改善する、
    - アウトカム報酬だと獲得が難しいことへの対策としてプロセス報酬を導入する
- AgenticなタスクにRLを適用する
    - ツール利用を行い環境と相互作用して探索を行い正しい行動を選択するようなタスク
    - ベースモデルが持っていない新しい能力を発見することが期待されている.利用方法を学習するプロセス。
    - 詳細は以前書いた記事を参照
https://zenn.dev/kuto5046/articles/agentic_rl_2025
    - SFTも有力な選択肢だが、アウトカム報酬により推論プロセスを学習可能である点や汎化性能に利点がありそう
    - SFTは記憶、RLは汎化を促すという論文もある
https://arxiv.org/abs/2501.17161