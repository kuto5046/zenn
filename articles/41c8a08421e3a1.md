---
title: "強化学習はLLMの新しい能力を「発見」しているのか？"
emoji: "🌊"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["RL", "LLM"]
published: false
---


## LLM研究におけるRLの躍進

2025年における強化学習(RL)の躍進は目覚ましいものがあります。中でも最も注目を集めたのは、2025年1月に発表されたDeepSeek-R1です。

https://arxiv.org/abs/2501.12948

DeepSeek-R1は、価値評価モデルを不要とするGRPOというRLアルゴリズムや、答えが一意に定まる問題に対して検証可能な報酬を利用することで報酬モデルを取り除いて学習コストを下げるといった具体策を提示しました。これらの手法により、RLがLLMの推論・汎化能力を飛躍的に向上させることを実証しています。

さらに、その過程ではahaモーメントと呼ばれる興味深い現象も観測されています。これはLLM自身が推論過程で気づきを得る事象であり、RLによって新しい推論能力を獲得しているように見えます。

しかし、RLが本当に新しい能力を獲得しているのかについては、研究者の間でいくつかの議論があります。本記事では、この興味深い議題について紹介していきます。

## RLの役割は「先鋭化」か「発見」か

RLがLLMに与える影響について、「先鋭化(Sharpening)」なのか「発見(Discovery)」なのかという議論が存在します。

先鋭化とは、ベースモデルが既に持っている能力を強調する役割を指します。一方、発見とは、ベースモデルにはない新しい能力を身につけさせることを意味します。なお、本記事では事前学習済みモデル(指示チューニングや選好チューニングを施していないもの)をベースモデルと呼ぶことにします。

先鋭化を主張する立場では、RLは一見新しい能力を獲得しているように見えるものの、実際にはベースモデルが潜在的に持っている能力を表出させているだけではないかと考えています。この議論については、まだ明確な決着がついているわけではありません。

以下では、それぞれの主張を支持するいくつかの論文を抜粋して紹介します。本記事では、以下のサーベイ論文やDeepResearch結果で紹介されている論文を参照しています。

https://arxiv.org/abs/2509.08827
https://chatgpt.com/s/dr_69422745b394819189a75584356b2cb3
https://gemini.google.com/share/69887002d84c

## 先鋭化しているだけという主張

### 論文1: RLVRは効率的なサンプリングに過ぎない

https://arxiv.org/abs/2504.13837

この論文は、DeepSeek-R1に端を発する強化学習と検証可能な報酬（RLVR）がLLMの推論能力を向上させるという一般的な認識に疑問を投げかけています。なお、検証可能な報酬とは、数学の問題に正解した場合に与える正解報酬のような、ルールベースで与えることができる報酬を指します。

本研究では、RLVRは下記の図に示すように単に既存の推論パターンをより効率的にサンプリングしているだけなのではないか、という仮説を検証しています。

![alt text](/images/41c8a08421e3a1/image1.png)

この仮説を検証するため、数学、コード生成、視覚推論といった複数のベンチマークで実験を実施しました。下記は一例として、複数の数学ベンチマークと複数のモデル種別ごとに、ベースモデルとRLモデルのpass@Kを比較したものです。pass@KとはK回推論を行い1回でも正解すれば正解とする指標です。
![alt text](/images/41c8a08421e3a1/image2.png)

検証の結果、pass@1ではRLが優れているものの、pass@KでKが大きくなるとベースモデルの方が性能が逆転することが明らかになりました。
これは、現在のRLVR手法はLLMの真に新しい推論能力を引き出すことに成功しておらず、ベースモデルのサンプリング効率を改善しているに留まっていることを示唆しています。本論文は、今回取り上げた議題を問題提起した重要な研究といえます。

### 論文2: サンプリングの工夫でRLと同等の性能を実現

https://arxiv.org/abs/2510.14901



この論文も、RLがモデルに「新しい能力」を与えているのではなく、単にベースモデルが持つ知識分布を「先鋭化（Sharpening）」させているだけではないかという仮説に基づく研究です。

本研究では、確率分布の先鋭化とサンプリングの工夫でRL相当の精度を実現することを試みました。具体的には、LLMが出力する確率分布を先鋭化するように確率分布を補正し、MCMCを利用してサンプリングする手法を提案しています。

検証の結果、推論時のサンプリング方法を工夫することで、RLを行わずとも複数のベンチマークにおいてpass@1指標がRLと同等かそれ以上の精度を達成できることを示しました。これは、RLがサンプリング効率を高める先鋭化の役割であることを示唆しています。

![alt text](/images/41c8a08421e3a1/image3.png)

### 小括

これらの主張は、現状のRLはあくまでサンプリング効率を高める先鋭化の役割が大きく、新しい能力の発見には至っていないというものです。

## 発見しているという主張

ここまではRLは先鋭化しているだけという主張の論文を紹介してきましたが、それに反論する論文も存在します。

### 論文1: 長期学習により新しい推論パスを発見

https://arxiv.org/abs/2505.24864

既存研究では、「RLはベースモデルに内在する正解を引き出しやすくするだけで、新しい推論能力を獲得するわけではない」という議論があります。しかし本研究は、適切なRL手法と十分な学習時間があれば、ベースモデルが全く解けないタスクでもRLによって解法を発見できることを実証しました。

具体的には、学習効率を高めるためにGRPOを改良したDAPOの論文で紹介されているテクニックを採用しています(なお、先鋭化を主張する論文はGRPOを使用しています)。また、長期的な学習を行う際に、参照モデルを初期のまま固定しておくとKLダイバージェンスの乖離が徐々に大きくなり学習が停滞してしまいます。そこで、参照モデルを定期的に更新することで学習の停滞を防ぎ、長期の学習を実現しました。

複数のベンチマークにおいてpass@Kを、ベースモデル、RLの学習途中、RLの学習完了後で比較したところ、タスクによって異なる傾向があることがわかりました。
![alt text](/images/41c8a08421e3a1/image4.png)
上記の図中でDiminishと記載されているベースモデルがすでに得意としているタスク(一部の数学タスク)においては、RLでpass@1が向上するもののKが大きいとベースモデルと性能が逆転しています。これは先鋭化の主張とも一致する結果です。

一方、Sustainedと記載されているコーディングのような複雑なタスクでは、学習が進むにつれて性能が上がり続けており、Kが大きくなってもベースモデルよりも性能が高いままです(ただし、この図だけ見るとSustainedでもKをもっと増やすと逆転しないか、というツッコミはありそうです)。

特に注目すべきは右下のdiceタスクです。ベースモデルはKを増やしても正答率が0であるのに対して、RLモデルは正答率が上昇しています。これはベースモデルの探索空間にはない新たな推論パスを発見していることを示唆しています。

この結果は、十分に難しく、探索を必要とするタスクにおいては、長時間の強化学習がモデルの推論の境界を押し広げ、新たな解法を発見し続けることができることを示唆しています。



### 論文2: 知識の論理的結合による推論の発見

https://arxiv.org/abs/2509.25123

この研究では、タスクに対する個別の知識は持っているが、それらを組み合わせて推論する能力は持っていない状態を意図的に作り出しました。具体的には、ベースモデルに対して複数の文字列変換関数f(x)をファインチューニングで学習させ、それぞれの関数を合成したもの h(x)=f1(f2(x)) を解くタスクを設定しました。合成関数の組み合わせ数に応じて難易度が上がるようになっています。

![alt text](/images/41c8a08421e3a1/image6.png)

実験の結果、難易度が低いものに対しては先鋭化の傾向が見られますが、難易度が高くなると、ベースモデルでは解けないもののRLでは正答率が上昇することが確認されました。ここでの主張は「新しい知識」の獲得ではなく、既存の知識断片を論理的に結合し、問題解決のための推論を発見させた点にあるといえます。

![alt text](/images/41c8a08421e3a1/image5.png)

## 総括

これまで紹介してきた研究を踏まえると、RLの役割は2者択一ではなく、先鋭化と発見の両方を兼ね備えていると考えられます。

基本的なタスクや条件では先鋭化の役割が大きい傾向にあります。一方で、一定の条件を満たすタスクにおいては、学習条件を整えることで発見も発生していることが示唆されています。

## 個人的解釈

ここからは個人的な解釈になります。

### なぜ先鋭化の傾向が出やすいのか

発見ではなく先鋭化の傾向が出やすいのは、以下の2つの理由が考えられます。

まず、難易度が低いタスクの場合、推論回数を増やせばベースモデルでも解けるため、新しい推論は必要ありません。したがって、発見はそもそも起こり得ないといえます。

一方、難易度が高いタスクの場合、推論回数を増やしてもベースモデルでは解けません。これはRLの学習過程において報酬を獲得できないことを意味します。そのため、RLを適用しても学習が進まず、やはり発見が起こりません。

### 発見を促すための条件

では、発見を促すにはどうすればよいのでしょうか。難易度が高いタスクに対して、Pro RLで示されているように学習条件を整えて学習効率を高めた状態で、学習ステップ数を十分増やすことが重要です。これにより確率的に報酬を獲得でき、発見が促されると考えられます。

実際、先鋭化を主張している研究自体も、「現行のRL手法では新能力創出に至っていない」という結論に加え、**「将来的にこの可能性を引き出すには訓練のさらなるスケール化やマルチターンの環境インタラクションが鍵になる」**と述べています。

このことから、Agentic RLのように探索が必要となる問題設定においては、RLの有効性は高いと考えています。ゲームAIでは、全く能力がない状態からRLによって報酬信号だけで最適行動を学習できるように、探索を通じた学習が可能です。

### 先鋭化の役割が期待される場合

ベースモデルでも推論回数を重ねれば解くことができるタスクに対してRLを適用する場合、先鋭化の役割が期待されます。この場合、少ない推論回数で正解に辿り着くことができる点、つまり推論効率の改善に価値があります。

### 発見の役割が期待される場合

発見の役割が期待されるのは、主に以下の2つのケースです。

**1. ベースモデルでは解けない難しいタスクへの適用**

問題設定として、難易度が高く複雑性のあるタスクに対してRLを適用する場合です。この場合、以下のような工夫が重要になります。

- アルゴリズムの工夫で学習効率を改善する
- アウトカム報酬だと獲得が難しいことへの対策として、プロセス報酬を導入する

**2. AgenticなタスクへのRLの適用**

ツール利用を行い環境と相互作用して探索を行い、正しい行動を選択するようなタスクです。このようなタスクでは、ベースモデルが持っていない新しい能力を発見することが期待されており、ツールの利用方法を学習するプロセスが重要になります。詳細は以前書いた記事を参照してください。

https://zenn.dev/kuto5046/articles/agentic_rl_2025

なお、SFTも有力な選択肢ですが、RLはアウトカム報酬により推論プロセスを学習可能である点や汎化性能において利点があると考えられます。SFTは記憶を促し、RLは汎化を促すという論文も存在します。

https://arxiv.org/abs/2501.17161