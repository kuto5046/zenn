---
title: "強化学習はLLMの新しい能力を『発見』しているのか？"
emoji: "💡"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["RL", "LLM"]
published: false
---


## LLM研究におけるRLの躍進

2025年における強化学習(Reinforcement Learning, RL)の躍進は目覚ましいものがあります。中でも最も注目を集めたのは、2025年1月に発表されたDeepSeek-R1です。

https://arxiv.org/abs/2501.12948

DeepSeek-R1は、価値評価モデルを不要とするGRPOというRLアルゴリズムや、答えが一意に定まる問題に対して検証可能な報酬を利用することで報酬モデルを取り除いて学習コストを下げるといった具体策を提示しました。これらの手法により、RLがLLMの推論・汎化能力を飛躍的に向上させることを実証しています。さらに、その過程ではahaモーメントと呼ばれる興味深い現象も観測されています。これはLLM自身が推論過程で気づきを得る事象であり、RLによって新しい推論能力を獲得しているように見えます。
しかし、RLが本当に新しい能力を獲得しているのかについては議論があり、トピックとして面白いなと思ったので紹介していきます。

## RLの役割は『先鋭化』か『発見』か

RLがLLMに与える影響は『先鋭化(Sharpening)』と『発見(Discovery)』のどちらなのかという議論が存在します。
- 先鋭化: ベースモデルが既に持っている能力を強調する役割
- 発見: ベースモデルにはない新しい能力を身につける役割

なお、本記事では事前学習済みモデル(指示チューニングや選好チューニングを施していないもの)をベースモデルと呼ぶことにします。
先鋭化を主張する立場の研究では、RLは一見新しい能力を獲得しているように見えるものの、実際には**ベースモデルが潜在的に持っている能力を表出させているだけではないか**と考えられています。
この記事では、それぞれの主張を支持する論文を以下のサーベイ論文から抜粋して紹介します。
https://arxiv.org/abs/2509.08827

## 先鋭化を主張する研究

### 論文1: RLはサンプリングを効率化しているに過ぎない

https://arxiv.org/abs/2504.13837
こちらはDeepSeek-R1に端を発する強化学習と検証可能な報酬（RL with Verifiable Rewards, RLVR）がLLMの推論能力を向上させるという一般的な認識に疑問を投げかけた研究です。RLVRとは、数学の問題に正解した場合に与える正解報酬のようなルールベースで与えることができる報酬を指します。下記のイメージ図に示すように**RLVRは単にベースモデルの推論パスをより効率的にサンプリングしているだけなのではないか**、という仮説を検証しています。
![alt text](/images/41c8a08421e3a1/image1.png)

この仮説を検証するため、ベースモデルに対してRLを実施後に、数学、コード生成、視覚推論といった複数のベンチマークでpass@Kという指標を用いた実験を実施しました。
pass@Kとは、K回推論を行い1回でも正解すれば正解とする指標です。例えば、pass@1は1回の推論で正解できるかどうか(つまり一般的な正答率)を示し、pass@10は10回の推論で1回でも正解できるかどうかを示します。
![alt text](/images/41c8a08421e3a1/image2.png)
複数の数学ベンチマークと複数のモデル種別ごとに、ベースモデルとRLモデルのpass@Kを比較した結果を見ると、pass@1ではRL(赤)が優れているものの、推論回数Kが大きくなるとベースモデル(緑)の方が性能が高くなり、結果が逆転する傾向が見られました。
これは、現在のRLVR手法はLLMの真に新しい推論能力を生み出してはおらず、ベースモデルのサンプリング効率を改善しているに留まっていることを示唆しています。

### 論文2: サンプリングの工夫でRLと同等の性能を実現

https://arxiv.org/abs/2510.14901

こちらの研究でも上記と同様、RLがモデルに「新しい能力」を与えているのではなく、単にベースモデルが持つ知識分布を「先鋭化（Sharpening）」させているだけではないかという仮説に基づき、LLM出力の確率分布の先鋭化とサンプリングの工夫によってファインチューニングなしにpass@1指標でRL相当の精度を実現可能であることを示しました。これはRLによる推論性能の改善が先鋭化とサンプリングの工夫によって実現可能であり、RLは先鋭化しているに過ぎないことを示唆しています。
![](/images/41c8a08421e3a1/image3.png)


## 発見を主張する研究

ここまで紹介した研究を聞くと、「あれ、RLってベースモデルを先鋭化してるだけかも？」という気持ちになってきますが、それに反論する研究も存在しています。

### 論文1: RLで長期学習することで新しい推論パターンを発見

https://arxiv.org/abs/2505.24864

先鋭化を主張する上記で示した研究では、推論回数Kを増やせばベースモデルがRLの性能を上回る事象が確認されていましたが、ProRLでは適切なRL手法と十分な学習ステップ数があれば、ベースモデルが全く解けないタスクでもRLによって解法を発見できることを実証しました。
具体的には、[DAPO](https://arxiv.org/abs/2503.14476)という論文で紹介されている学習効率を高めるテクニックを採用しています。また長期間の学習を行う際に、参照モデルを初期のまま固定しておくとKLダイバージェンスの乖離が徐々に大きくなり学習が停滞する課題があるため参照モデルを定期的に更新するといった学習上の工夫を行っています。
複数のベンチマークにおいてpass@Kを、ベースモデル、RLの学習途中、RLの学習完了後の3つで比較したところ、タスクによって異なる傾向があることがわかりました。
![alt text](/images/41c8a08421e3a1/image4.png)
Diminish:ベースモデルがすでに得意としているタスク
- RLによりpass@1のスコアは向上するが、Kが大きいとベースモデルと性能が逆転している
- 先鋭化を主張している研究結果とも一致する

Sustained: 複雑なタスク
- 学習が進むにつれて性能が上がり続けている(オレンジ < 緑)
- Kが大きくなってもRLモデルの方がベースモデルよりも性能が高い

この結果を批判的に見るとK=256で終了しているからそのように見えるだけでKをさらに大きくすれば、ベースモデルが逆転するのでは？という疑念もありますが、本研究で注目すべきは図の右下のdiceタスクのような事例です。
diceタスクを含む一部のタスクでは推論回数Kを増やしてもベースモデルの正答率が常に0である一方でRLモデルは学習によって正答率が上昇していることがわかります。

このように本研究では、難易度が低めのタスクにおいて先鋭化の傾向があることを認めつつも、難易度が高いタスクにおいては学習がうまく進むように工夫することによってベースモデルでは解けない新たな解法を発見していることを示唆しています。

### 論文2: 知識の論理的結合による推論の発見

https://arxiv.org/abs/2509.25123

この研究では、タスクに対する個別の知識は持っているが、それらを組み合わせて推論する能力は持っていない状態を意図的に作り出しました。具体的には、ベースモデルに対して複数の文字列変換関数f(x)をファインチューニングで学習させ、それぞれの関数を合成したもの h(x)=f1(f2(x)) を解くタスクを設定しました。合成関数の組み合わせ数に応じて難易度が上がるようになっています。

![alt text](/images/41c8a08421e3a1/image6.png)

実験の結果、難易度が低いものに対しては先鋭化の傾向が見られますが、難易度が高くなると、ベースモデルでは解けないもののRLでは正答率が上昇することが確認されました。ここでの主張は「新しい知識」の獲得ではなく、既存の知識断片を論理的に結合し、問題解決のための推論を発見させた点にあるといえます。

![alt text](/images/41c8a08421e3a1/image5.png)

## 現時点での暫定的な結論
このトピックについては、まだ明確な決着がついているわけではありませんが、ここで紹介した研究の周囲も含めて見渡すと以下のような整理に現状ではなってると考えています。
- RLの役割は先鋭化 or 発見の2者択一ではなく両方の役割を兼ね備えている
- ベースモデルでも推論回数を増やせば解くことが可能なタスクにおいては先鋭化の役割が大きい傾向にある
- ベースモデルでも解くことが難しいような一定の条件を満たすタスクにおいては、学習条件を適切に整えることで発見も発生しうる
- いずれの場合でもベースモデルの性能にはある程度依存している。

## 個人的な解釈と学び
ここからはこのトピックに関して私が考えたことを述べてみます。あくまで現時点での個人的な解釈であり、誤っている点もあるかもしれないためご了承ください。

### なぜ先鋭化の傾向が出やすいのか

周辺の研究を見ていると、発見が発生してそうな事例もあるものの、RLは先鋭化の傾向が出やすい印象を持っています。発見ではなく先鋭化の傾向が出やすいのは、以下の2つの理由があると考えてます。
- 難易度が低いタスクの場合はベースモデルでも解けるためそもそも発見は起こらない。その結果、先鋭化として観測される
- 逆に難易度が高いタスクは、アウトカム報酬のみでは報酬が獲得できずRLの学習が進みにくい。そのため学習条件やタスク設定を工夫しないと発見が起こりにくい

### RLはどういう目的で使うべきか

#### 先鋭化を目的とする場合
ベースモデルでも推論回数を増やせば解くことができるタスクに対して、少ない推論回数で正解に辿り着けるようにしたい場合にRLが有効と考えられます。推論回数を減らしたいといったケースは多く存在すると考えられるため、先鋭化としての役割も重要であると考えています。
同様の目的として教師あり学習(Supervised Fine Tuning, SFT)も有力な選択肢としてあり、タスクによるという前提はありつつも、タスクの最終結果の評価のみで推論プロセスを学習可能である点や、過学習を抑え異なるタスクへの汎化能力が落ちにくい点はSFTと比較したときにRLが優れている点なのかなと考えています。

https://arxiv.org/abs/2501.17161

#### 発見を目的とする場合
1. ベースモデルでは解けない難しいタスクへの適用

記事の中でも紹介したように難易度が高いタスクに対して学習条件を工夫することで発見的な性能向上が発生する可能性があります。
難易度が高いタスクでは報酬獲得が難しいのが最大の課題であるため、以下のような工夫が必要になると考えています。これらはRL研究でずっと取り組まれている普遍的な課題でもあります。
- エントロピー正則化などの手法で方策の探索を促進する
- RLアルゴリズムを工夫して学習効率を高める
- プロセス報酬の導入してアウトカム報酬獲得のための補助を行う

2. マルチステップのエージェントタスクへの適用

LLMが取り組んでいた従来タスクはユーザプロンプトを入力として最適な文字列応答を生成するシングルステップのタスクが主でしたが、AIエージェントの登場に伴いマルチステップのタスクが増えてきています。ツール利用によって環境と相互作用して探索を行い、最適な行動を選択するエージェントタスクの学習は、本来のRLが得意とするタスクです。

実際、先鋭化を主張している論文でも、「現行のRL手法では新能力創出に至っていない」という結論に加え、「将来的にRLによる新能力創出の可能性を引き出すには訓練のさらなるスケール化やマルチターンの環境インタラクションが鍵になる」と述べています。
エージェント型強化学習(Agentic RL)については以前、周辺研究を調査した記事を書いたので興味があれば読んでみてください。
https://zenn.dev/kuto5046/articles/agentic_rl_2025


## おわりに
本記事では、LLMに対するRLの役割が「先鋭化」なのか「発見」なのかという議論を紹介しました。
