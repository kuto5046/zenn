---
title: "強化学習はReasoning性能を向上させているのか"
emoji: "🐡"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: []
published: false
---

2025年9月10日にarxivで公開されたReasoning modelに対する強化学習のサーベイ論文を読みました。この4章の議論が面白かったので紹介します。
[A Survey of Reinforcement Learning for Large
Reasoning Models](https://arxiv.org/abs/2509.08827)

## 強化学習によるReasoning性能の向上
近年では大規模推論モデル（Large Reasoning Models, LRM）に対する強化学習(RL)が新たな潮流となりつつあります。これは従来のRLHFのようにモデルの挙動を人間に合わせるだけでなく、推論能力そのものを強化することを目的としています。
OpenAIの「o1」やDeepSeek社の「R1」といった事例では、検証可能な報酬（Verifiable Reward）を用いた強化学習（RLVR）によって、モデルに長尺の推論（計画、内省、自己修正など）を可能にさせることに成功しました。例えば数学問答では正答か否か、コード生成ではユニットテストの合否といった自動的に確認可能な基準を報酬とすることで、モデルの推論プロセス全体にフィードバックを与えています。
- OpenAI o1
    - 追加のRL訓練や推論時に「考える」ステップを増やすことで性能がなめらかに向上することが示され、事前学習規模以外の新たな性能向上軸が明らかになりました。
- DeepSeek-R1
    - DeepSeek-R1では数学の厳密な正解判定やコードのコンパイル結果・テスト結果を報酬とするルールベース手法を採用し、GRPOを使った大規模RL（特にグループ相対方策最適化: GRPO）よってモデルに高度な推論行動を誘発したと報告しています。このように推論そのものを鍛える訓練により、モデルは中間的な思考過程（Chain of Thought; CoT）を自発的に生成・評価・修正する能力を獲得し、推論に時間（計算量）をかけるほど成果が向上する傾向が確認されています。

## 強化学習はReasoning性能を向上させているのか
上記を読むと、強化学習がReasoning性能を向上させているように見えます。
ただしこれに関しては2つの考え方があるようです。

1.RLは既存能力の強調（Sharpening）に過ぎない
2.RLは新たな能力の発見（Discovery）を可能にする